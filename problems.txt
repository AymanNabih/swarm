Возникли два варианта, как можно распараллелить сам алгоритм.

Первый вариант, это распаралелить вычисление, которые происходят в алгоритме. Т.к. numpy очень хорошо справляется с вычислениями, то этот способ эффективен только при ну очень больших входных данных (и то он не всегда существенно выигрывает, т.к. на "жизнь" одного потока выделяется немало памяти), ну или нужна какая-нибудь особая функция, для которой нужно найти экстремум (например, функция, у которой входные параметры это слова), с которой numpy не справится и нужно будет использовать циклы для подсчёта нового положения агента. Тут возникает вопрос, какую функцию можно взять для проверки данного предположения?

Второй вариант заключается в том, чтобы распаралелить самих агентов, т.е. изначально есть N агентов алгоритма, и они разбиваются на n подгрупп, которые независимо друг от друга ищут экстремум функции. В конце каждая подгруппа агентов выдаёт свой лучший результат, из которых выбирается глобальное лучше положение. Вопрос: имеет ли вообще данный вариант смысл? Я предполагаю, что он хорош только в том случае, когда у функции помимо глобального экстремума есть много локальных экстремумов, куда могут "упасть" агенты. Поэтому, шанс, что алгоритм найдёт именно глобальный экстремум, возрастает. Но опять же, пригодится ли этот в будущем? 

В распараллеливание создания объектов, использующие роевые алгоритмы, вопросов нет. При "прямом" создании 1000 объектов, которые используют pso, было затрачено 104с, а при "распараллеленом" на 8 ядерном компьютере - 57с.

Наблюдения можно просмотреть в файле draft.py.
